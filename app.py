import streamlit as st
import google.generativeai as genai
from pypdf import PdfReader
from streamlit_gsheets import GSheetsConnection
import pandas as pd
import yfinance as yf
import re
import json
from datetime import datetime
import time

# --- 1. åˆå§‹åŒ–è¨­å®š ---
st.set_page_config(page_title="AI é£†è‚¡åµæ¸¬èˆ‡é€±å ±è³‡æ–™åº«", layout="wide", page_icon="ğŸ“ˆ")

try:
    genai.configure(api_key=st.secrets["GEMINI_KEY"])
    # æ¡ç”¨æœ€æ–°çš„ Gemini æ¨¡å‹
    model = genai.GenerativeModel('gemini-2.0-flash') 
    conn = st.connection("gsheets", type=GSheetsConnection)
except Exception as e:
    st.error(f"åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ Secrets è¨­å®š: {e}")
    st.stop()

# --- 2. æ ¸å¿ƒé‚è¼¯å‡½å¼ ---

def extract_stock_ids(text):
    """å¾æ–‡å­—ä¸­æå– 4 ä½æ•¸å°è‚¡ä»£ç¢¼ï¼Œä¸¦è‡ªå‹•è™•ç†éå­—ä¸²è³‡æ–™"""
    if not isinstance(text, str):
        text = str(text) if text is not None else ""
    return re.findall(r'\b\d{4}\b', text)
def get_stock_perf(sid):
    """ç²å–å°è‚¡å³æ™‚è¡Œæƒ…èˆ‡æ¼²è·Œå¹…"""
    try:
        suffix = ".TW" if int(sid) < 9000 else ".TWO"
        t = yf.Ticker(f"{sid}{suffix}")
        h = t.history(period="1mo")
        if h.empty: return None
        cur = h['Close'].iloc[-1]
        chg = ((cur - h['Close'].iloc[0]) / h['Close'].iloc[0]) * 100
        return {"price": cur, "change": chg}
    except: return None

def check_breakout_dna(sid):
    """
    é£†è‚¡èµ·æ¼² DNA åµæ¸¬ï¼š
    1. å‡ç·šç³¾çµåº¦ < 3.5% (5/10/20 MA)
    2. æˆäº¤é‡èç¸® < 0.75 (ç›¸è¼ƒæ–¼ 20 æ—¥å‡é‡)
    3. è‚¡åƒ¹åœ¨ 60MA (å­£ç·š) ä¹‹ä¸Š (å¤šé ­æ¶æ§‹)
    """
    try:
        suffix = ".TW" if int(sid) < 9000 else ".TWO"
        t = yf.Ticker(f"{sid}{suffix}")
        df = t.history(period="65d") 
        if len(df) < 60: return None
        
        # è¨ˆç®—æŒ‡æ¨™
        df['MA5'] = df['Close'].rolling(5).mean()
        df['MA10'] = df['Close'].rolling(10).mean()
        df['MA20'] = df['Close'].rolling(20).mean()
        df['MA60'] = df['Close'].rolling(60).mean()
        
        last = df.iloc[-1]
        vol_avg20 = df['Volume'].rolling(20).mean().iloc[-1]
        
        # 1. å‡ç·šç³¾çµåº¦
        ma_list = [last['MA5'], last['MA10'], last['MA20']]
        ma_gap = (max(ma_list) / min(ma_list) - 1) * 100
        
        # 2. é‡æ¯”
        v_ratio = last['Volume'] / vol_avg20 if vol_avg20 > 0 else 1
        
        # 3. åˆ¤å®šæ¢ä»¶
        is_ready = (ma_gap < 3.5) and (v_ratio < 0.75) and (last['Close'] > last['MA60'])
        
        return {
            "sid": sid,
            "price": round(last['Close'], 2),
            "gap": round(ma_gap, 2),
            "v_ratio": round(v_ratio, 2),
            "is_ready": is_ready
        }
    except:
        return None

# --- 3. App ä»‹é¢ä½ˆå±€ ---
st.title("ğŸ“‚ å°ˆæ¥­é€±å ± JSON çµæ§‹åŒ–èˆ‡é£†è‚¡ DNA åµæ¸¬ç³»çµ±")
tab1, tab2, tab3, tab4 = st.tabs(["ğŸš€ é€±å ±è§£æ", "ğŸ“… æ­·å²è¨ºæ–·", "ğŸ“š è³‡æ–™åº«æ˜ç´°", "âš¡ é£†è‚¡åµæ¸¬å™¨"])

# é å…ˆè®€å–è³‡æ–™åº«
try:
    db = conn.read(worksheet="Sheet1")
except:
    db = pd.DataFrame()

# --- Tab 1: é€±å ±è§£æ ---
with tab1:
    up_col1, up_col2 = st.columns([3, 1])
    with up_col1:
        uploaded_file = st.file_uploader("ä¸Šå‚³é€±å ± PDF", type="pdf")
    with up_col2:
        re_analyze = st.button("ğŸ”„ é‡æ–°åˆ†æ")

    if uploaded_file:
        if 'json_analysis' not in st.session_state or re_analyze:
            with st.spinner('Gemini æ­£åœ¨é€²è¡Œæ·±åº¦çµæ§‹åŒ–æƒæ...'):
                reader = PdfReader(uploaded_file)
                text = "".join([p.extract_text() for p in reader.pages if p.extract_text()])
                
                prompt = f"""
                ä½ æ˜¯ä¸€ä½å°ˆæ¥­è‚¡ç¥¨åˆ†æå“¡ã€‚è«‹æ·±å…¥åˆ†æé€±å ±å…§å®¹ã€‚
                è«‹è¼¸å‡ºä¸€å€‹ JSON æ ¼å¼åˆ—è¡¨ï¼Œä¸åŒ…å« Markdown æ¨™ç±¤æˆ–é¡å¤–æ–‡å­—ã€‚
                æ ¼å¼ï¼š[{{"é¡Œæ": "åç¨±", "åŸå› ": "10å­—å…§", "æ¨™çš„": "4ä½æ•¸ä»£ç¢¼+åç¨±"}}]
                é€±å ±å…§å®¹ï¼š{text[:15000]}
                """
                res = model.generate_content(prompt)
                st.session_state.json_analysis = res.text
                
                date_match = re.search(r'\d{4}-\d{2}-\d{2}', uploaded_file.name)
                st.session_state.rep_date = date_match.group(0) if date_match else datetime.now().strftime("%Y-%m-%d")

        if 'json_analysis' in st.session_state:
            st.markdown(f"### ğŸ“‹ {st.session_state.rep_date} çµæ§‹åŒ–çµæœ")
            st.code(st.session_state.json_analysis, language='json')

            if st.button("ğŸ“¥ ç¢ºèªå­˜å…¥ Google Sheets"):
                try:
                    raw_str = st.session_state.json_analysis.replace('```json', '').replace('```', '').strip()
                    data_list = json.loads(raw_str)
                    new_df = pd.DataFrame(data_list)
                    new_df['æ—¥æœŸ'] = st.session_state.rep_date
                    final_df = pd.concat([db, new_df[['æ—¥æœŸ', 'é¡Œæ', 'åŸå› ', 'æ¨™çš„']]], ignore_index=True)
                    conn.update(worksheet="Sheet1", data=final_df)
                    st.success("âœ… æ•¸æ“šå·²æˆåŠŸåŒæ­¥è‡³é›²ç«¯è³‡æ–™åº«ï¼")
                except Exception as e:
                    st.error(f"å­˜å…¥å¤±æ•—: {e}")

# --- Tab 2: æ­·å²å›æº¯ ---
with tab2:
    st.subheader("ğŸ“Š æ­·å²æ¨™é¡Œå‹•èƒ½æ¯”å°")
    if not db.empty:
        dates = db['æ—¥æœŸ'].unique()[::-1]
        sel_date = st.selectbox("é¸æ“‡å›æº¯æ—¥æœŸ", dates)
        sub_df = db[db['æ—¥æœŸ'] == sel_date]
        
        for _, row in sub_df.iterrows():
            with st.expander(f"ğŸ“Œ {row['é¡Œæ']} - {row['æ¨™çš„']}"):
                sids = extract_stock_ids(row['æ¨™çš„'])
                for sid in sids:
                    perf = get_stock_perf(sid)
                    if perf:
                        c1, c2 = st.columns(2)
                        c1.metric(f"{sid} ç¾åƒ¹", f"{perf['price']:.2f}")
                        c2.metric("è¿‘ä¸€æœˆå¹…åº¦", f"{perf['change']:.2f}%")
    else:
        st.info("è³‡æ–™åº«å°šç„¡è³‡æ–™ã€‚")

# --- Tab 3: è³‡æ–™åº«æ˜ç´° ---
with tab3:
    st.subheader("ğŸ“š é›²ç«¯è³‡æ–™åº«æ¸…å–®")
    # ä¿®æ­£ï¼š2026 Streamlit è¦ç¯„ï¼Œä½¿ç”¨ width="stretch" å¡«æ»¿å¯¬åº¦
    if not db.empty:
        st.dataframe(db, width="stretch")
    else:
        st.info("ç›®å‰è³‡æ–™åº«å…§æ²’æœ‰æ•¸æ“šã€‚")

# --- Tab 4: âš¡ é£†è‚¡åµæ¸¬å™¨ (é›™æ¨¡å¼ï¼šè³‡æ–™åº« vs å…¨å°è‚¡) ---
with tab4:
    st.subheader("âš¡ å°‹æ‰¾èµ·æ¼²é»ï¼šå‡ç·šç³¾çµ + çª’æ¯é‡åµæ¸¬")
    
    # æ¨¡å¼é¸æ“‡
    scan_mode = st.radio("é¸æ“‡æƒæç¯„åœ", ["è³‡æ–™åº«å…§çš„é¡Œæè‚¡", "æƒæå…¨å°è‚¡ (1101~9960)"], horizontal=True)
    
    # é–€æª»è¨­å®š (è®“ä½¿ç”¨è€…å¾®èª¿ï¼Œå¢åŠ éˆæ´»æ€§)
    col_p1, col_p2 = st.columns(2)
    with col_p1:
        gap_threshold = st.slider("å‡ç·šç³¾çµé–€æª» (%)", 1.0, 5.0, 3.5, 0.5, help="æ•¸å€¼è¶Šå°ä»£è¡¨ç±Œç¢¼è¶Šé›†ä¸­")
    with col_p2:
        vol_threshold = st.slider("çª’æ¯é‡é–€æª» (å€)", 0.1, 1.0, 0.75, 0.05, help="æ•¸å€¼è¶Šå°ä»£è¡¨è³£å£“è¶Šä¹¾æ·¨")

    # æº–å‚™æƒææ¸…å–®
    search_list = []
    if scan_mode == "è³‡æ–™åº«å…§çš„é¡Œæè‚¡":
        if not db.empty:
            all_sids = []
            for s in db['æ¨™çš„']: 
                clean_s = str(s) if pd.notna(s) else ""
                all_sids.extend(extract_stock_ids(clean_s))
            search_list = list(set(all_sids))
            st.info(f"ğŸ” æ¨¡å¼ï¼šè³‡æ–™åº«ç²¾æº–ç›£æ§ (å…± {len(search_list)} æª”)")
        else:
            st.warning("è³‡æ–™åº«ç›®å‰æ˜¯ç©ºçš„ï¼Œè«‹å…ˆè§£æé€±å ± PDFã€‚")
    else:
        # å»ºç«‹å…¨å°è‚¡ä»£ç¢¼æ±  (1101~9960)
        # è¨»ï¼šé€™æœƒåŒ…å«ä¸€äº›ä¸å­˜åœ¨çš„è™Ÿç¢¼ï¼Œä½† check_breakout_dna æœƒè‡ªå‹•è·³é
        search_list = [str(i) for i in range(1101, 9961)]
        st.info("ğŸš€ æ¨¡å¼ï¼šå…¨å°è‚¡å¤§æ•¸æ“šæƒæ (é è¨ˆéœ€æ™‚ 2-5 åˆ†é˜)")

    # åŸ·è¡ŒæƒææŒ‰éˆ•
    if st.button("ğŸ é–‹å§‹ DNA ç¯©é¸"):
        if not search_list:
            st.error("æƒææ¸…å–®ç‚ºç©ºï¼Œè«‹ç¢ºèªè³‡æ–™åº«æ•¸æ“šã€‚")
        else:
            results = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            start_time = time.time()
            total_count = len(search_list)
            
            # éæ­·æ¸…å–®
            for i, sid in enumerate(search_list):
                # æ¯ 10 æª”æ›´æ–°ä¸€æ¬¡æ–‡å­—ï¼Œé¿å…ç•«é¢é–ƒçˆéå¿«
                if i % 10 == 0 or i == total_count - 1:
                    status_text.text(f"æ­£åœ¨åˆ†æ: {sid} ({i}/{total_count})")
                
                # å‘¼å«æŠ€è¡“é¢æª¢æŸ¥å‡½å¼
                res = check_breakout_dna(sid)
                
                # ç¬¦åˆä½¿ç”¨è€…è¨­å®šçš„é–€æª»æ‰ç´å…¥
                if res and res['gap'] <= gap_threshold and res['v_ratio'] <= vol_threshold:
                    # check_breakout_dna å…§éƒ¨å·²åŒ…å« price > MA60 åˆ¤æ–·
                    results.append(res)
                
                # æ›´æ–°é€²åº¦æ¢
                progress_bar.progress((i + 1) / total_count)
            
            end_time = time.time()
            status_text.success(f"æƒæå®Œæˆï¼ç¸½è€—æ™‚: {int(end_time - start_time)} ç§’")
            
            if results:
                st.success(f"ğŸŠ ç™¼ç¾ {len(results)} æª”ç¬¦åˆèµ·æ¼²ç‰¹å¾µçš„æ¨™çš„ï¼")
                
                # æ•´ç†æˆ DataFrame
                res_df = pd.DataFrame(results).drop(columns=['is_ready'])
                res_df.columns = ['è‚¡ç¥¨ä»£è™Ÿ', 'ç›®å‰åƒ¹æ ¼', 'å‡ç·šç³¾çµåº¦(%)', 'æˆäº¤é‡æ¯”']
                
                # ä¾æ“šã€å‡ç·šç³¾çµåº¦ã€æ’åº (è¶Šå°è¶Šå¥½)
                res_df = res_df.sort_values(by='å‡ç·šç³¾çµåº¦(%)', ascending=True)
                
                # é¡¯ç¤ºè¡¨æ ¼ (ä½¿ç”¨ 2026 æœ€æ–°è¦ç¯„ width="stretch")
                st.dataframe(res_df, width="stretch")
                
                st.caption("ğŸ’¡ å°ˆæ¥­å»ºè­°ï¼šå„ªå…ˆè§€å¯Ÿå‡ç·šç³¾çµåº¦ < 2% ä¸”æˆäº¤é‡æ¯” < 0.5 çš„æ¨™çš„ï¼Œä»£è¡¨çˆ†ç™¼åŠ›æœ€å¼·ã€‚")
            else:
                st.info("ç›®å‰é¸å®šç¯„åœå…§ï¼Œå°šç„¡æ¨™çš„åŒæ™‚æ»¿è¶³è¨­å®šçš„èµ·æ¼²æ¢ä»¶ã€‚")
