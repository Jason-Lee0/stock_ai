import streamlit as st
import google.generativeai as genai
from pypdf import PdfReader
from streamlit_gsheets import GSheetsConnection
import pandas as pd
import yfinance as yf
import re
import json
from datetime import datetime
import time

# --- 1. åˆå§‹åŒ–è¨­å®š ---
st.set_page_config(page_title="AI é£†è‚¡åµæ¸¬ç³»çµ± Pro", layout="wide", page_icon="ğŸš€")

try:
    genai.configure(api_key=st.secrets["GEMINI_KEY"])
    model = genai.GenerativeModel('gemini-2.0-flash') 
    conn = st.connection("gsheets", type=GSheetsConnection)
except Exception as e:
    st.error(f"åˆå§‹åŒ–å¤±æ•—: {e}")
    st.stop()

# --- 2. æ ¸å¿ƒæŠ€è¡“åˆ†æå‡½å¼ ---

def extract_stock_ids(text):
    """æå– 4 ä½æ•¸ä»£ç¢¼ä¸¦ç¢ºä¿ç‚ºå­—ä¸²"""
    if not isinstance(text, str):
        text = str(text) if pd.notna(text) else ""
    return re.findall(r'\b\d{4}\b', text)

def check_breakout_dna(sid):
    """
    æ ¸å¿ƒåµæ¸¬å¼•æ“ï¼šMAç³¾çµ + é‡ç¸® + å­£ç·šä¸Šæš + MACDå‹•èƒ½
    """
    try:
        suffix = ".TW" if int(sid) < 9000 else ".TWO"
        df = yf.Ticker(f"{sid}{suffix}").history(period="80d")
        if len(df) < 65: return None
        
        # A. å‡ç·šè¨ˆç®—
        df['MA5'] = df['Close'].rolling(5).mean()
        df['MA10'] = df['Close'].rolling(10).mean()
        df['MA20'] = df['Close'].rolling(20).mean()
        df['MA60'] = df['Close'].rolling(60).mean()
        
        # B. MACD è¨ˆç®— (ç”¨ä¾†åˆ¤å®šå‹•èƒ½æ˜¯å¦è½‰å¼·ï¼Œé¿é–‹ç›¤æ•´å¤±çœŸ)
        exp1 = df['Close'].ewm(span=12, adjust=False).mean()
        exp2 = df['Close'].ewm(span=26, adjust=False).mean()
        df['DIF'] = exp1 - exp2
        df['DEA'] = df['DIF'].ewm(span=9, adjust=False).mean()
        df['MACD_Hist'] = df['DIF'] - df['DEA']
        
        last = df.iloc[-1]
        prev = df.iloc[-2]
        
        # C. åˆ¤å®šæ¢ä»¶
        # 1. å‡ç·šç³¾çµåº¦ (5/10/20 MA)
        ma_list = [last['MA5'], last['MA10'], last['MA20']]
        ma_gap = (max(ma_list) / min(ma_list) - 1) * 100
        
        # 2. çª’æ¯é‡æ¯”
        vol_avg20 = df['Volume'].rolling(20).mean().iloc[-1]
        v_ratio = last['Volume'] / vol_avg20 if vol_avg20 > 0 else 1
        
        # 3. å­£ç·šè¶¨å‹¢ (ä»Šæ—¥ MA60 > 5æ—¥å‰ MA60)
        is_ma60_up = last['MA60'] > df['MA60'].iloc[-5]
        
        # 4. MACD å‹•èƒ½ (æŸ±ç‹€é«”å¾€ä¸Šèµ° = è½‰å¼·è¨Šè™Ÿ)
        momentum_score = 1 if last['MACD_Hist'] > prev['MACD_Hist'] else 0
        
        # ç¶œåˆè©•åˆ† (åŸºç¤é–€æª»ï¼šç«™åœ¨å­£ç·šä¸Š & å­£ç·šä¸Šæš)
        is_ready = (last['Close'] > last['MA60']) and is_ma60_up
        
        return {
            "sid": sid,
            "price": round(last['Close'], 2),
            "gap": round(ma_gap, 2),
            "v_ratio": round(v_ratio, 2),
            "trend": "ğŸ“ˆ ä¸Šæš" if is_ma60_up else "ğŸ“‰ ä¸‹å½",
            "signal": "ğŸ”¥ è½‰å¼·" if momentum_score else "â³ æ•´ç†",
            "is_ready": is_ready
        }
    except: return None

# --- 3. App ä»‹é¢ ---
st.title("ğŸ“ˆ å°è‚¡èµ·æ¼²é» DNA è‡ªå‹•åŒ–æƒæå„€")
tab1, tab2, tab3, tab4 = st.tabs(["ğŸš€ é€±å ±è§£æ", "ğŸ“… æ­·å²è¨ºæ–·", "ğŸ“š è³‡æ–™åº«æ˜ç´°", "âš¡ é£†è‚¡åµæ¸¬å™¨"])

# é å–è³‡æ–™åº«
try:
    db = conn.read(worksheet="Sheet1")
except:
    db = pd.DataFrame()

# --- Tab 1 & 2 & 3 (ä¿æŒåŸæœ‰åŠŸèƒ½) ---
with tab1:
    up_col1, up_col2 = st.columns([3, 1])
    with up_col1: uploaded_file = st.file_uploader("ä¸Šå‚³é€±å ± PDF", type="pdf")
    with up_col2: re_analyze = st.button("ğŸ”„ é‡æ–°åˆ†æ")
    if uploaded_file and ('json_analysis' not in st.session_state or re_analyze):
        with st.spinner('Gemini åˆ†æä¸­...'):
            reader = PdfReader(uploaded_file)
            text = "".join([p.extract_text() for p in reader.pages if p.extract_text()])
            prompt = f"è«‹å°‡é€±å ±å…§å®¹è½‰ç‚º JSON åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{{\"é¡Œæ\": \"\", \"åŸå› \": \"\", \"æ¨™çš„\": \"ä»£ç¢¼+åç¨±\"}}]ã€‚å…§å®¹ï¼š{text[:15000]}"
            res = model.generate_content(prompt)
            st.session_state.json_analysis = res.text
            st.session_state.rep_date = datetime.now().strftime("%Y-%m-%d")
    if 'json_analysis' in st.session_state:
        st.code(st.session_state.json_analysis, language='json')
        if st.button("ğŸ“¥ å­˜å…¥é›²ç«¯è³‡æ–™åº«"):
            raw = st.session_state.json_analysis.replace('```json', '').replace('```', '').strip()
            new_data = pd.DataFrame(json.loads(raw))
            new_data['æ—¥æœŸ'] = st.session_state.rep_date
            conn.update(worksheet="Sheet1", data=pd.concat([db, new_data], ignore_index=True))
            st.success("å­˜å„²æˆåŠŸï¼")

with tab3:
    st.subheader("ğŸ“š é›²ç«¯è³‡æ–™åº«æ˜ç´°")
    st.dataframe(db, width="stretch")

# --- Tab 4: âš¡ é£†è‚¡åµæ¸¬å™¨ (æ ¸å¿ƒåˆä½µç‰ˆ) ---
with tab4:
    st.subheader("âš¡ ç¯©é¸æ¢ä»¶è¨­å®š")
    
    col_a, col_b = st.columns(2)
    with col_a:
        scan_mode = st.radio("æƒæç¯„åœ", ["è³‡æ–™åº«é¡Œæè‚¡", "å…¨å°è‚¡ (1101~9960)"], horizontal=True)
    with col_b:
        gap_limit = st.slider("å‡ç·šç³¾çµé–€æª» (%)", 1.0, 5.0, 3.5)
        vol_limit = st.slider("çª’æ¯é‡é–€æª» (å€)", 0.1, 1.0, 0.75)

    if st.button("ğŸš€ é–‹å§‹åŸ·è¡Œèµ·æ¼²é»åµæ¸¬"):
        # æº–å‚™æ¸…å–®
        if scan_mode == "è³‡æ–™åº«é¡Œæè‚¡":
            sids = []
            if not db.empty:
                for s in db['æ¨™çš„']: sids.extend(extract_stock_ids(s))
                search_list = list(set(sids))
            else: search_list = []
        else:
            # æ’é™¤ ETF (00xx)ï¼Œå¾ 1101 æ™®é€šè‚¡é–‹å§‹
            search_list = [str(i) for i in range(1101, 9961)]

        if not search_list:
            st.warning("è«‹å…ˆç¢ºä¿è³‡æ–™åº«æœ‰æ•¸æ“šæˆ–é¸æ“‡å…¨å°è‚¡æ¨¡å¼ã€‚")
        else:
            hits = []
            progress = st.progress(0)
            status = st.empty()
            start = time.time()

            for i, sid in enumerate(search_list):
                if i % 10 == 0: status.text(f"æƒæä¸­: {sid} ({i}/{len(search_list)})")
                res = check_breakout_dna(sid)
                
                # æœ€çµ‚éæ¿¾é‚è¼¯ï¼šç³¾çµåº¦ã€é‡ç¸®ã€ä¸”ç¬¦åˆ is_ready (å­£ç·šä¸Šæš+è‚¡åƒ¹åœ¨ç·šä¸Š)
                if res and res['gap'] <= gap_limit and res['v_ratio'] <= vol_limit and res['is_ready']:
                    hits.append(res)
                progress.progress((i + 1) / len(search_list))
            
            status.success(f"æƒæå®Œæˆï¼è€—æ™‚ {int(time.time()-start)} ç§’")
            
            if hits:
                st.success(f"ğŸŠ ç™¼ç¾ {len(hits)} æª”æ½›åŠ›æ¨™çš„ï¼")
                df_res = pd.DataFrame(hits).drop(columns=['is_ready'])
                df_res.columns = ['ä»£è™Ÿ', 'ç¾åƒ¹', 'ç³¾çµ(%)', 'é‡æ¯”', 'å­£ç·šè¶¨å‹¢', 'å‹•èƒ½è¨Šè™Ÿ']
                st.dataframe(df_res.sort_values('ç³¾çµ(%)'), width="stretch")
                
                
            else:
                st.info("ç›®å‰å°šæœªç™¼ç¾å®Œå…¨ç¬¦åˆ DNA æ¢ä»¶çš„æ¨™çš„ã€‚")
